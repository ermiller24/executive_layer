Executive Layer design document

# CONCEPT

The Executive Layer, or ExL, is going to be an operability layer that lives on top of an arbitray llm. It exists to provide executive thinking, course correction, contextual knowledge, and advanced planning and reasoning capabilities. At the same time it should act as a drop-in replacement for any LLM model, implementing an OpenAI-compatible API. In theory users should be able to set their environment variables for the configuration and otherwise not have to worry.

The ExL operates by running two LLM instances simultaneously, which we'll refer to as layers. The first layer is the "speaker". It is the forward-facing model that the user interacts with, providing chat, tool calls, etc. The second LLM is the actual "executive" layer. This layer provides support for the speaker. Also included in the ExL will be a knowledge graph implemented in Neo4j (for now) and a vector store database.


# FLOW
The basic flow of the ExL will be as follows. The user prompts the system using an OpenAI-compatible endpoint. The prompt is first passed to the vector store, and relevant context is retrieved if any is available. This context along with the user's prompt and a system prompt are fed into the LLM, and it begins streaming back results. These are immediately streamed back out to the user, so from their point of view the process is no slower than any normal LLM call.

At the same time, the executive layer takes the user's query and begins searching its knowledge graph for relevant information. This information is compiled into a "knowledge document". At this point it is likely that the speaker is still speaking, so the executive takes the user's prompt plus the information it has gathered from the knowledge graph and verifies that the speaker is on the right track. There are a couple of possibilities here:

1. The speaker is making good progress and not making mistakes. The executive layer decides not to interrupt. Meanwhile, the knowledge document put together by the executive is entered into the vector store if the embedding of the user prompt is sufficiently "new". Subsequent calls to the speaker with substantially similar prompts will be able to retrieve the compiled document and include it in the initial prompt, providing a speed-up as the system learns.
2. The speaker is making good progress but has made some errors or gotten slightly off track, but the context it got from the vector store is good. The executive prompts the speaker with the original query and any insights in order to get the speaker back on track, but doesn't significantly interfere. If the embedding of the user prompt is sufficiently new, the knowledge document is entered into the vector store.
3. The speaker is making progress but is making errors because the initial context was incorrect or not applicable. The executive interrupts the speaker with the new knowledge document, but lets it continue. The new knowledge document is entered into the vector store. If the context from the vector store is objectively wrong or misleading, it is deleted, otherwise it is kept.
4. The speaker is substantially wrong or has seriously deviated from its task. The executive stops the speaker and restarts it from the beginning, with the new knowledge document and updated instructions. The vector store is updated as usual.

Now the executive does another pass looking at the knowledge graph. It makes updates to the graph as necessary based on information learned from the conversation. Then it does another evaluation pass at the speaker's output to verify that the speaker is on the right track, and the cycle continues.

From the user's perspective, it looks as if the system is simply streaming LLM thoughts. All of this interaction is invisible to them. They simply see an LLM that has better knowledge, learns in real time, and can course-correct as necessary.


# IMPLEMENTATION

Langchain's integrations already allow it to access a wide variety of LLM models through their respective python APIs and return a standard Runnable object that include batch, streaming, etc. The first thing to implement is essentially a bridge layer docker container that uses langchain Runnable objects to replicate the OpenAI API as close as possible. Features that aren't built into the langchain integration can either be disabled or be simply passed straight through to the original API while presenting a warning to the user. That's something we need to decide on.

Once we have that implemented, we need to implement the executive LLM. The executive runs in parallel with the speaker and has an "interrupt" tool and a "restart" tool. If the executive calls for interrupt, the executive's interruption is passed through to the user and then the speaker resumes with its work and the new information. If the executive calls to restart, a message is passed through to the user that the LLM is restarting its though process and then the speaker starts streaming again from the beginning. It is transparent to the user when the executive interrupts if they understand the underlying architecture, but in practice they still just keep receiving streaming chunks.

Then we implement the Neo4J knowledge graph. An implementation of this already exists in my work/chatbot/ directory, it can essentially be copied into the new ExL codebase. Only the executive has access to this system, and it is responsible for both querying it and managing it. Test to make sure that the speaker can be given a bad prompt and have it corrected by the knowledge in the knowledge graph by giving the speaker a fake prompt with bad data and loading the correct data into the knowledge graph, and then run and tweak prompts until the executive is properly correcting the speaker. Test with both "slightly wrong" prompts to focus on interruptions and "severely wrong" prompts for restarts.

Then we implement the vector store. In theory the vector store should almost be self-managing, as we are just inserting the knowledge documents generated by the executive with the user prompt as the key as long as the prompt embedding lies sufficiently far from the retrieved embedding. The executive may need to have some ability to forcibly discard information from the vector store in case the information is really bad. The vector store is accessed immediately upon getting the prompt, and its response is included directly in the speaker's system prompt.


# EXTENSIONS

One possible extension is to add a managed docker instance to the system to act as a sandbox. The executive could use this to build longer documents, execute computations, or do project planning or file storage. The executive could, for example, see that the speaker is trying to write python code using a certain method of an object and decide to use its sandbox to check that the object actually has that method, adding that information into its knowledge graph and passing it back to the speaker as necessary. It may instead be better to leave the docker sandbox out and package it as an MCP tool for the client, like we currently have in work/chatbot. That sort of depends on how effective the executive is at its other tasks and if it has extra bandwidth. Do not implement this until we have come to a decision on this design choice.

Another possible extension is to let the executive handle prompt extension. When the context window of the speaker starts to get long, the executive can prepare a summary of what has happened so far and then reset the speaker. This way the speaker's context window is automatically managed and it can go far beyond the standard conversation length.

A third extension is to allow different models for the speaker and the executive. Of particular interest is a chat model for the speaker and a thinking model for the executive. That would result in the user getting the more personable chat model responses but still getting the benefits of the thinking models.

Another extension that might be of interest is tagging the executive's responses as thought and passing them in their entirety back to the user. That lets the user see everything that is going on, but would muddy the response a bit.

We also could implement a queue system for the executive's llm calls. That would allow us to have multiple different models running as executives simultaneously, each with a different focus, while the speaker continues. This could prove to be overwhelming or confusing and should not be included until we can test it.

There is also a possibility of externally hosting both the knowledge graph and the vector store. This would allow multiple instances to be running while all sharing the same knowledge base and learning in real time. As the size of the system grows we have to worry that too many systems might make the knowledge graph too muddy, and so we implement separate knowledge graph managers from the executive layers. This is more of an enterprise-grade problem, so we don't need to do this unless we get good traction.

Another idea is advanced planning capabilities by having the executive keep track of both a design and a progress document, like we have in this repo, when the prompt would necessitate it. In theory the speaker could have its prompts reset by the executive and the executive prompt could reset using the design and progress document, allowing infinite output context.
Executive Layer design document

# CONCEPT

The Executive Layer, or ExL, is an operability layer that lives on top of an arbitrary LLM. It exists to provide executive thinking, course correction, contextual knowledge, and advanced planning and reasoning capabilities. At the same time it should act as a drop-in replacement for any LLM model, implementing an OpenAI-compatible API. In theory users should be able to set their environment variables for the configuration and otherwise not have to worry.

The ExL operates by running two LLM instances simultaneously, which we'll refer to as layers. The first layer is the "speaker". It is the forward-facing model that the user interacts with, providing chat, tool calls, etc. The second LLM is the actual "executive" layer. This layer provides support for the speaker. Also included in the ExL will be a knowledge graph implemented in Neo4j, which will incorporate vector embeddings for semantic search capabilities.


# FLOW

The basic flow of the ExL will be as follows. The user prompts the system using an OpenAI-compatible endpoint. The prompt is passed to the speaker LLM along with a system prompt, and it begins streaming back results. These are immediately streamed back out to the user, so from their point of view the process is no slower than any normal LLM call.

At the same time, the executive layer takes the user's query and begins monitoring the speaker's output. The executive has access to the knowledge graph and can search it for relevant information using both semantic (vector-based) and structured queries. If the executive determines that the speaker needs additional information or correction, it can interrupt the speaker with relevant information.

There are several possible scenarios:

1. The speaker is making good progress and not making mistakes. The executive layer decides not to interrupt. Meanwhile, the executive may update the knowledge graph with new information learned from the conversation if appropriate.

2. The speaker is making good progress but has made some errors or gotten slightly off track. The executive interrupts the speaker with the original query and any insights in order to get the speaker back on track. The speaker then continues with this new information, correcting itself in real-time.

3. The speaker is making progress but is making errors because it lacks important information. The executive interrupts the speaker with a knowledge document compiled from the knowledge graph. The speaker then continues with this new information, incorporating it into its ongoing response.

The executive continues to monitor the speaker's output and make updates to the knowledge graph as necessary based on information learned from the conversation. This cycle continues throughout the interaction.

From the user's perspective, it looks as if the system is simply streaming LLM thoughts. All of this interaction is invisible to them. They simply see an LLM that has better knowledge, learns in real time, and can course-correct as necessary.


# IMPLEMENTATION

Langchain's integrations already allow it to access a wide variety of LLM models through their respective python APIs and return a standard Runnable object that include batch, streaming, etc. The first thing to implement is essentially a bridge layer docker container that uses langchain Runnable objects to replicate the OpenAI API as close as possible. Features that aren't built into the langchain integration can either be disabled or be simply passed straight through to the original API while presenting a warning to the user.

Once we have that implemented, we need to implement the executive LLM. The executive runs in parallel with the speaker and has an "interrupt" tool. If the executive calls for interrupt, the executive's interruption is passed through to the user and then the speaker resumes with its work and the new information. It is transparent to the user when the executive interrupts if they understand the underlying architecture, but in practice they still just keep receiving streaming chunks.

Then we implement the Neo4J knowledge graph with vector embeddings. We'll leverage Neo4j's vector capabilities to enable semantic search within the structured knowledge graph. This will allow the speaker to efficiently query the knowledge graph using both semantic similarity and explicit relationships. The executive will be responsible for maintaining and organizing this knowledge graph, ensuring that information is properly structured and accessible.

For the speaker, we'll implement tool calls that allow it to directly query the knowledge graph. These tools will enable the speaker to search for information based on semantic similarity, explicit relationships, or a combination of both. This approach eliminates the need for a separate vector store and provides a more unified and efficient knowledge retrieval system.

We'll also implement functionality for the executive to compile project plans into the knowledge graph when a large project is initiated. This will ensure that when a user returns to work on a project, the speaker has immediate access to existing design and progress information. This approach allows for project continuity without requiring special modes or context window management, as the relevant information is stored in a structured format within the knowledge graph.


# EXTENSIONS

## External Knowledge Infrastructure

Externally hosting the knowledge graph. This would allow multiple instances to be running while all sharing the same knowledge base and learning in real time. As the size of the system grows we have to worry that too many systems might make the knowledge graph too muddy, and so we implement separate knowledge graph managers from the executive layers. This is more of an enterprise-grade problem, so we don't need to do this unless we get good traction.
